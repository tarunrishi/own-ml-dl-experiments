{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9576b6",
   "metadata": {},
   "source": [
    "# Stable Diffusion for Text-to-Image Generation\n",
    "### Stable Diffusion is a cutting-edge generative AI model designed to create high-quality images from textual descriptions using a diffusion process. It assumes that images can be generated by iteratively refining random noise into coherent visuals guided by text prompts, making it highly effective for tasks like generating custom visuals when there is a strong alignment between text and image features. This flexibility enables rapid prototyping and creative applications, ideal for enhancing user engagement in scenarios like financial dashboards. However, its computational intensity and sensitivity to prompt quality can limit its performance without proper tuning.\n",
    "\n",
    "### Use Stable Diffusion as a baseline model for text-to-image generation, refining it based on the use case.\n",
    "\n",
    "| Aspect | Details |\n",
    "| :- | :- |\n",
    "| Use For           | Text-to-image generation (e.g., creating financial charts from prompts)<br>Can be extended to image inpainting or style transfer |\n",
    "| Key Assumptions   | - Strong alignment between text prompts and image features<br>- Sufficient data for learning text-image relationships<br>- Availability of computational resources for training |\n",
    "| Advantages        | - Generates high-quality, diverse images<br>- Flexible for creative applications<br>- Pre-trained models available for quick deployment |\n",
    "| Disadvantages     | - Computationally intensive during training and inference<br>- Prone to generating irrelevant or low-quality images without fine-tuning<br>- Sensitive to prompt quality and specificity |\n",
    "| Avoid When        | - Very small datasets with limited samples<br>- Low computational resources<br>- Applications requiring real-time generation |\n",
    "| Real-World Use Case | Financial dashboard generation (e.g., creating budget charts from text prompts)<br>Marketing content creation (e.g., generating visuals for financial reports) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb32f0",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "### Generate small images of fashion items from text prompts (e.g., \"Generate a black T-shirt\") using a simplified Stable Diffusion-inspired model trained on the Fashion MNIST dataset from Kaggle. This is a text-to-image synthesis task where the model learns to refine noise into images based on text labels, optimized for memory efficiency.\n",
    "\n",
    "Use the Fashion MNIST dataset from Kaggle: https://www.kaggle.com/datasets/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b760b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:44:43.342961: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 15:44:43.354582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751467483.365119 2721161 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751467483.368421 2721161 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751467483.378161 2721161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751467483.378174 2721161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751467483.378175 2721161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751467483.378176 2721161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-02 15:44:43.381446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06966fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = './19_stablediffusion'\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "model_dir = os.path.join(project_dir, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ad9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gpu():\n",
    "    ## tf version\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "    #### GPU Optimisation code ####\n",
    "    print(\"GPUs:\",tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "    print(\"GPUs: Allocate GPU Memory and create a new session\")\n",
    "\n",
    "    # Get the GPU memory fraction to allocate\n",
    "    gpu_memory_fraction = 0.5\n",
    "\n",
    "    # Create GPUOptions with the fraction of GPU memory to allocate\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "\n",
    "    # Create a session with the GPUOptions\n",
    "    session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b21966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPUs: Allocate GPU Memory and create a new session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751467487.237764 2721161 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5119 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "setup_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22de69f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# check TensorFlow version and handle AUTOTUNE compatibility\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "if hasattr(tf.data, 'AUTOTUNE'):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "else:\n",
    "    AUTOTUNE = 2  # Fallback to a fixed prefetch buffer size (e.g., 2)\n",
    "    print(\"Warning: tf.data.AUTOTUNE not available, using fallback value 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cbebcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Fashion MNIST dataset from Kaggle\n",
    "def load_fashionmnist():\n",
    "    # assuming dataset is downloaded to data_dir (update path as needed)\n",
    "    train_data = pd.read_csv(os.path.join(data_dir, 'fashion-mnist_train.csv'))\n",
    "    test_data = pd.read_csv(os.path.join(data_dir, 'fashion-mnist_test.csv'))\n",
    "    \n",
    "    # extract features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values / 255.0\n",
    "    y_train = to_categorical(train_data['label'].values)\n",
    "    X_test = test_data.drop('label', axis=1).values / 255.0\n",
    "    y_test = to_categorical(test_data['label'].values)\n",
    "    \n",
    "    # reshape to 28x28x1 images\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6937e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data with text labels\n",
    "def preprocess_data(X, y):\n",
    "    # map labels to text prompts \n",
    "    label_to_text = {\n",
    "        0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n",
    "        5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"\n",
    "    }\n",
    "    text_labels = np.array([label_to_text[np.argmax(yi)] for yi in y])\n",
    "    return X, text_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e407c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from text labels\n",
    "def create_vocabulary(text_labels):\n",
    "    vocab = {word: i+1 for i, word in enumerate(set(word for text in text_labels for word in text.split()))}\n",
    "    vocab['<PAD>'] = 0\n",
    "    return vocab\n",
    "\n",
    "# simple text encoder with tokenization\n",
    "def tokenize_text(texts, vocab, max_length=5):\n",
    "    tokenized = []\n",
    "    for text in texts:\n",
    "        # Ensure text is a string and handle NumPy string objects\n",
    "        text_str = str(text) if hasattr(text, 'decode') else str(text)\n",
    "        tokens = [vocab.get(word, 0) for word in text_str.split()[:int(max_length)]]\n",
    "        tokens = tokens + [0] * (int(max_length) - len(tokens)) if len(tokens) < int(max_length) else tokens[:int(max_length)]\n",
    "        tokenized.append(tokens)\n",
    "    return tf.convert_to_tensor(tokenized, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694418ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a basic U-Net for diffusion\n",
    "class SimpleUNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleUNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))  # Reduce to 14x14\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')\n",
    "        self.upconv = tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding='same', output_padding=1, activation='relu')  # Upsample to 28x28\n",
    "        self.conv3 = tf.keras.layers.Conv2D(1, 3, padding='same', activation='sigmoid')\n",
    "        self.text_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.text_dense = tf.keras.layers.Dense(14 * 14, activation='relu')  # Match reduced size\n",
    "        self.text_up = tf.keras.layers.UpSampling2D(size=(2, 2))  # Upsample text to 28x28\n",
    "\n",
    "    def call(self, inputs):\n",
    "        noise, text_emb = inputs\n",
    "        # Process text embedding to reduce sequence dimension\n",
    "        text = self.text_pool(text_emb)  # Reduce (32, 5, 128) to (32, 128)\n",
    "        text = self.text_dense(text)  # Map to (32, 196)\n",
    "        text = tf.reshape(text, (-1, 14, 14, 1))  # Reshape to 14x14\n",
    "        text = self.text_up(text)  # Upsample to 28x28\n",
    "        \n",
    "        # U-Net: Refine noise with text conditioning\n",
    "        x = self.conv1(noise)  # (32, 28, 28, 16)\n",
    "        x = self.pool(x)  # (32, 14, 14, 16)\n",
    "        x = self.conv2(x)  # (32, 14, 14, 32)\n",
    "        x = self.upconv(x)  # (32, 28, 28, 16)\n",
    "        x = tf.keras.layers.Concatenate(axis=-1)([x, text])  # (32, 28, 28, 17)\n",
    "        x = self.conv3(x)  # Reduce to 1 channel\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c269e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up diffusion parameters\n",
    "timesteps = 20  # reduced for efficiency\n",
    "beta = np.linspace(0.0001, 0.02, timesteps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_cumprod = np.cumprod(alpha, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55a35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise to images\n",
    "def add_noise(images, timestep):\n",
    "    noise = tf.random.normal(shape=tf.shape(images), dtype=tf.float32)\n",
    "    sqrt_alpha_cumprod = tf.sqrt(alpha_cumprod[timestep])\n",
    "    sqrt_one_minus_alpha_cumprod = tf.sqrt(1 - alpha_cumprod[timestep])\n",
    "    noisy_images = sqrt_alpha_cumprod * images + sqrt_one_minus_alpha_cumprod * noise\n",
    "    return noisy_images, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee63c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751467487.297303 2721161 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5119 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# initialize and compile the model\n",
    "model = SimpleUNet()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4db7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = load_fashionmnist()\n",
    "X_train, text_train = preprocess_data(X_train, y_train)\n",
    "X_test, text_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# create vocabulary and initialize text encoder\n",
    "vocab = create_vocabulary(text_train)\n",
    "text_encoder = tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=128)\n",
    "\n",
    "# convert text to embeddings for training\n",
    "text_train_emb = text_encoder(tokenize_text(text_train, vocab))\n",
    "text_test_emb = text_encoder(tokenize_text(text_test, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5441e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751467491.568818 2721320 service.cc:152] XLA service 0x7fc1ec003e70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1751467491.568848 2721320 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2025-07-02 15:44:51.587607: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1751467491.703388 2721320 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-07-02 15:44:52.407752: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_557', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-02 15:44:52.547957: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_326', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1751467493.643823 2721320 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps):\n\u001b[32m      8\u001b[39m     noisy_images, noise = add_noise(batch_images, t)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoisy_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m flat_outputs = \u001b[38;5;28mself\u001b[39m.call_flat(*args)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpack_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py:450\u001b[39m, in \u001b[36mFunctionType.pack_output\u001b[39m\u001b[34m(self, flat_values)\u001b[39m\n\u001b[32m    448\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not pack outputs for undefined output type.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflat_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/framework/type_spec.py:262\u001b[39m, in \u001b[36mTypeSpec.from_tensors\u001b[39m\u001b[34m(self, tensors)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;129m@doc_controls\u001b[39m.do_not_doc_inheritable\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_tensors\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensors):\n\u001b[32m    259\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"See TraceType base class for details. Do not override.\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m   components = nest.map_structure(\n\u001b[32m    261\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m spec: spec.from_tensors(tensors),\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_component_specs\u001b[49m\n\u001b[32m    263\u001b[39m   )\n\u001b[32m    264\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._from_components(components)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/own-ml-dl-experiments/lib/python3.12/site-packages/tensorflow/python/data/ops/optional_ops.py:244\u001b[39m, in \u001b[36mOptionalSpec._component_specs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_component_specs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mtensor_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensorSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# training loop with diffusion steps\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}/10\")\n",
    "    for i in range(0, len(X_train), 32):  # Batch manually due to small dataset\n",
    "        batch_images = X_train[i:i+32]\n",
    "        batch_text = text_train_emb[i:i+32]\n",
    "        for t in range(timesteps):\n",
    "            noisy_images, noise = add_noise(batch_images, t)\n",
    "            model.fit([noisy_images, batch_text], batch_images, batch_size=32, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate an image from a text prompt\n",
    "def generate_image(model, text_emb, timesteps):\n",
    "    noise = tf.random.normal(shape=(1, 28, 28, 1), dtype=tf.float32)\n",
    "    for t in reversed(range(timesteps)):\n",
    "        predicted_noise = model.predict([noise, text_emb], verbose=0)\n",
    "        noise = (noise - beta[t] * predicted_noise) / tf.sqrt(alpha[t])\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with a sample prompt\n",
    "sample_prompt = tokenize_text([\"black T-shirt\"])\n",
    "sample_emb = text_encoder(sample_prompt)\n",
    "generated_image = generate_image(model, sample_emb, timesteps)\n",
    "\n",
    "# display the result\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.title(\"Generated Fashion Item\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for later use\n",
    "model_filename = os.path.join(model_dir, 'sd_model.h5')\n",
    "model.save(model)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# load and test the saved model\n",
    "loaded_model = tf.keras.models.load_model(os.path.join(model_dir, 'model.h5'), custom_objects={'SimpleUNet': SimpleUNet})\n",
    "test_image = generate_image(loaded_model, sample_emb, timesteps)\n",
    "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
    "plt.title(\"Test from Loaded Model\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "own-ml-dl-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
