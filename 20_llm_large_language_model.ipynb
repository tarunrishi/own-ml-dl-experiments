{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37233961",
   "metadata": {},
   "source": [
    "# Large Language Model (LLM) from Scratch\n",
    "### A Large Language Model (LLM) is a foundational generative AI model designed to understand and generate human-like text by learning patterns from large text corpora. It assumes that text sequences follow probabilistic patterns that can be modeled with transformer architectures, making it highly effective for tasks like text generation when trained on diverse data. This flexibility enables rapid prototyping for applications like advice generation. However, its computational intensity and need for large datasets can limit performance without significant resources.\n",
    "\n",
    "### Use LLM as a baseline model for text generation, refining it based on the use case.\n",
    "\n",
    "| Aspect | Details |\n",
    "| :- | :- |\n",
    "| Use For           | Text generation (e.g., generating financial advice)<br>Can be extended to question answering or summarization |\n",
    "| Key Assumptions   | - Text sequences follow learnable patterns<br>- Sufficient data for training<br>- Adequate computational resources |\n",
    "| Advantages        | - Generates coherent, context-aware text<br>- Highly flexible for various tasks<br>- Customizable for specific domains |\n",
    "| Disadvantages     | - Computationally intensive to train<br>- Prone to overfitting without regularization<br>- Requires large datasets |\n",
    "| Avoid When        | - Limited computational resources<br>- Small or noisy datasets<br>- Real-time applications without pre-training |\n",
    "| Real-World Use Case | Advice generation (e.g., creating budget tips from prompts)<br>Summarisation <br>Chatbot responses for user queries |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0721f5",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "### Generate coherent text sequences using a small transformer-based LLM trained from scratch on the OpenWebText dataset. This is a text generation task where the model learns to predict the next token in a sequence, optimized for memory efficiency.\n",
    "\n",
    "Using OpenWebText dataset and filter for financial advice-related texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00277b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b910a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = './20_llm_large_language_model'\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "model_dir = os.path.join(project_dir, 'model')\n",
    "log_dir = os.path.join(project_dir, 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b634c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gpu():\n",
    "    ## tf version\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "    #### GPU Optimisation code ####\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        print(\"GPU available:\", physical_devices)\n",
    "    else:\n",
    "        print(\"No GPU found, using CPU.\")\n",
    "        return\n",
    "\n",
    "    print(\"GPUs: Allocate GPU Memory and create a new session\")\n",
    "\n",
    "    # Get the GPU memory fraction to allocate\n",
    "    gpu_memory_fraction = 0.5\n",
    "\n",
    "    # Create GPUOptions with the fraction of GPU memory to allocate\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "\n",
    "    # Create a session with the GPUOptions\n",
    "    session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download NLTK data for tokenization\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OpenWebText dataset (using Hugging Face datasets for simplicity)\n",
    "def load_openwebtext_financial(batch_size=1000):\n",
    "    \"\"\"\n",
    "    Load OpenWebText dataset and filter for financial advice-related texts.\n",
    "    Uses keyword matching to identify financial content.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"openwebtext\", split=\"train\", streaming=True)\n",
    "    financial_keywords = [\"finance\", \"investment\", \"budget\", \"money\", \"savings\", \"stocks\", \"retirement\", \"debt\", \"credit\", \"tax\"]\n",
    "\n",
    "    def is_financial(text):\n",
    "        return any(keyword in text.lower() for keyword in financial_keywords)\n",
    "\n",
    "    financial_texts = []\n",
    "    for example in dataset:\n",
    "        if is_financial(example['text']):\n",
    "            financial_texts.append(example['text'])\n",
    "            if len(financial_texts) >= batch_size:\n",
    "                yield financial_texts\n",
    "                financial_texts = []\n",
    "    if financial_texts:\n",
    "        yield financial_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5afbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and create vocabulary\n",
    "def preprocess_text(texts, vocab_size=1000, seq_length=20, max_sequences=10000):\n",
    "    \"\"\"\n",
    "    Tokenize and create sequences, limiting to max_sequences for memory control.\n",
    "    \n",
    "    Parameters:\n",
    "    - texts: List of text strings\n",
    "    - vocab_size: Number of unique words to keep (default 1000)\n",
    "    - seq_length: Length of each sequence (default 20)\n",
    "    - max_sequences: Maximum number of sequences to generate (default 2000)\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    word_counts = Counter(all_tokens)\n",
    "    most_common_words = [word for word, _ in word_counts.most_common(vocab_size - 1)]\n",
    "    vocab = {word: idx + 1 for idx, word in enumerate(most_common_words)}\n",
    "    vocab['<PAD>'] = 0\n",
    "\n",
    "    token_ids = [vocab.get(token, 0) for token in all_tokens]\n",
    "    sequences = [token_ids[i:i + seq_length] for i in range(0, len(token_ids) - seq_length, seq_length)]\n",
    "    sequences = sequences[:max_sequences]\n",
    "    sequences = [seq + [0] * (seq_length - len(seq)) for seq in sequences]\n",
    "    return np.array(sequences), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21612e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample text\n",
    "def generate_text(model, vocab, seed_text, seq_length=20, length=50, temperature=0.7):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    tokens = [vocab.get(word, 0) for word in word_tokenize(seed_text.lower())]\n",
    "    tokens = tokens[:seq_length - 1] + [0] * (seq_length - 1 - len(tokens))\n",
    "    for _ in range(length):\n",
    "        input_seq = np.array([tokens[-(seq_length - 1):]])\n",
    "        pred = model.predict(input_seq, verbose=0)\n",
    "        # apply temperature sampling\n",
    "        pred = np.log(pred + 1e-10) / temperature  # avoid log(0)\n",
    "        pred = np.exp(pred) / np.sum(np.exp(pred))  # softmax with temperature\n",
    "        next_token = np.argmax(pred[0, -1])  # still use argmax for simplicity, can use random.choice for true sampling\n",
    "        tokens.append(next_token)\n",
    "    return ' '.join([inv_vocab.get(t, '<PAD>') for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity\n",
    "def calculate_perplexity(model, X, y):\n",
    "    loss = model.evaluate(X, y, verbose=0)[0]\n",
    "    return np.exp(loss)\n",
    "\n",
    "# calculate BLEU score\n",
    "def calculate_bleu(model, vocab, seed_text, reference_texts):\n",
    "    generated = generate_text(model, vocab, seed_text).split()\n",
    "    references = [ref.split() for ref in reference_texts]\n",
    "    return sentence_bleu(references, generated)\n",
    "\n",
    "# calculate ROUGE score\n",
    "def calculate_rouge(model, vocab, seed_text, reference_texts):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    generated = generate_text(model, vocab, seed_text)\n",
    "    scores = [scorer.score(ref, generated) for ref in reference_texts]\n",
    "    avg_scores = {\n",
    "        'rouge1': np.mean([s['rouge1'].fmeasure for s in scores]),\n",
    "        'rouge2': np.mean([s['rouge2'].fmeasure for s in scores])\n",
    "    }\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e250c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess data in batches\n",
    "batch_size = 50000  # Adjust based on memory constraints\n",
    "financial_batches = load_openwebtext_financial(batch_size=batch_size)\n",
    "\n",
    "# load and preprocess data\n",
    "texts = next(financial_batches)\n",
    "sequences, vocab = preprocess_text(texts, vocab_size=1000, seq_length=20, max_sequences=10000)\n",
    "seq_length = sequences.shape[1]\n",
    "X_train = sequences[:, :-1]\n",
    "y_train = tf.keras.utils.to_categorical(sequences[:, 1:], num_classes=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3784388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple transformer model\n",
    "def build_transformer(vocab_size, seq_length, d_model=32, num_heads=2):\n",
    "    inputs = Input(shape=(seq_length,))\n",
    "    x = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    x = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(x, x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(0.85)(x)  # Increased dropout rate\n",
    "    x = Dense(d_model, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.15))(x)  # Stronger L2 regularization\n",
    "    x = Dropout(0.85)(x)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and compile model\n",
    "model = build_transformer(vocab_size=len(vocab), seq_length=X_train.shape[1])\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4, clipvalue=1.0),  # Clip gradients at 1.0\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=22, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=1000, validation_split=0.2, verbose=1,\n",
    "                    callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "print(\"\\n### Model Evaluation\")\n",
    "\n",
    "# accuracy from training history\n",
    "final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "print(f\"Final Validation Accuracy: {final_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc02ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "\n",
    "# Perplexity\n",
    "perplexity = calculate_perplexity(model, X_train, y_train)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "# BLEU score\n",
    "reference_texts = [\"save money by budgeting monthly\", \"invest in stocks for long-term growth\"]\n",
    "bleu_score = calculate_bleu(model, vocab, \"Financial advice for\", reference_texts)\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "\n",
    "# ROUGE score\n",
    "rouge_scores = calculate_rouge(model, vocab, \"Financial advice for\", reference_texts)\n",
    "print(f\"ROUGE Scores: ROUGE-1: {rouge_scores['rouge1']:.2f}, ROUGE-2: {rouge_scores['rouge2']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06493c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_prompts = [\n",
    "    \"Financial advice for investing\",\n",
    "    \"How to save money\",\n",
    "    \"Investing in stocks is\",\n",
    "    \"Budgeting tips for\",\n",
    "    \"Retirement planning involves\",\n",
    "    \"Managing debt effectively requires\",\n",
    "    \"Credit score improvement strategies include\",\n",
    "    \"Emergency funds should cover\",\n",
    "    \"Tax-saving investments can\",\n",
    "    \"Financial independence means\"\n",
    "]\n",
    "\n",
    "generated_texts = [generate_text(model, vocab, prompt) for prompt in seed_prompts]\n",
    "results = pd.DataFrame({\n",
    "    'Seed Prompt': seed_prompts,\n",
    "    'Generated Text': generated_texts\n",
    "})\n",
    "print(\"Sample Test Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(os.path.join(model_dir, 'llm_from_scratch.h5'))\n",
    "print(\"Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "own-ml-dl-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
